title: 统计学习方法（笔记）-朴素贝叶斯法
mathjax: true
date: 2016-11-03 20:52:47
categories: 统计学习
description:
tags:
- 统计学习
- 笔记
- 机器学习
- 朴素贝叶斯
---

朴素贝叶斯（naïve Bayes）法是基于被也是定力与特征条件独立假设的分布方法。首先对于训练数据集，基于特征条件独立假设学习输入输出的联合概率分布，然后对于给定的输入，利用贝叶斯定理求出后验概率最大的输出。

# 朴素贝叶斯法的学习与分类

朴素贝叶斯法是典型的生成学习方法。利用训练数据学习$P(X|Y)$和$P(Y)$的估计，得到联合概率分布

$$P(X,Y) = P(X)P(X|Y)$$

概率估计方法可以是极大似然估计或者贝叶斯估计。

朴素贝叶斯法的基本假设使条件独立性。

$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\\ =\prod \limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$$

条件独立假设使朴素贝叶斯法变得简单，但是有时候会牺牲一定的分类准确率。

朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测，将输入的 x 分到后验概率最大的类 y：

$$y=\arg \max _{c_k}P(Y=c_k)\prod \limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$$

后验概率最大等价于0-1函数损失时的期望风险最小化。

# 朴素贝叶斯法的参数估计

在朴素贝叶斯法中，学习即估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$

## 极大似然估计

先验概率$P(Y=c_k)$的极大似然估计是：

$$P(Y=c_k)=\frac{\sum \limits_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,...,K$$

设第 j 个特征$x^{(j)}$可能取值集合为${a_{j1},a_{j2},...,a_{jS_j} }$，则条件概率为：

$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum \limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum \limits_{i=1}^NI(y_i=c_k)}\\ j=1,2,...,n;\quad l=1,2,...S;\quad k=1,2,...,K$$

## 贝叶斯估计

极大似然估计可能会出现估计的概率值为 0 的情况，这会影响到后验概率的计算结果。可以使用贝叶斯估计来解决这一问题。贝叶斯估计即加入一个系数防止0情况出现。


$$P_\lambda(Y=c_k)=\frac{\sum \limits_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda},\quad k=1,2,...,K$$

$$P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum \limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum \limits_{i=1}^NI(y_i=c_k)+S_j\lambda}\\ j=1,2,...,n;\quad l=1,2,...S;\quad k=1,2,...,K$$

上面两式中，$\lambda\geq0$，当取1时，称为拉普拉斯平滑。


